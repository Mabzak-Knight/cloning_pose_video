{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "oo6a17iOKinO",
        "8Syx9AN0XQLP"
      ],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOoAZjBNlViq2WgrCDJTD5a",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mabzak-Knight/cloning_pose_video/blob/main/demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installer\n",
        "https://github.com/yoyo-nb/Thin-Plate-Spline-Motion-Model"
      ],
      "metadata": {
        "id": "oo6a17iOKinO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-fnt0PmCKOK0",
        "outputId": "edd7a380-9f9a-4e2c-c005-bd623204ddae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Thin-Plate-Spline-Motion-Model'...\n",
            "remote: Enumerating objects: 115, done.\u001b[K\n",
            "remote: Counting objects: 100% (65/65), done.\u001b[K\n",
            "remote: Compressing objects: 100% (36/36), done.\u001b[K\n",
            "remote: Total 115 (delta 43), reused 31 (delta 29), pack-reused 50\u001b[K\n",
            "Receiving objects: 100% (115/115), 32.66 MiB | 21.78 MiB/s, done.\n",
            "Resolving deltas: 100% (51/51), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/yoyo-nb/Thin-Plate-Spline-Motion-Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd Thin-Plate-Spline-Motion-Model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k-F_roXyKb9I",
        "outputId": "028b2f99-8caf-485a-b8d5-0e1b0dbe2510"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Thin-Plate-Spline-Motion-Model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GxNIlUBbKfIo",
        "outputId": "17fb0c30-9587-4e4b-dcb8-c9392281346b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting cffi==1.14.6 (from -r requirements.txt (line 1))\n",
            "  Downloading cffi-1.14.6.tar.gz (475 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/475.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/475.7 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.7/475.7 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting cycler==0.10.0 (from -r requirements.txt (line 2))\n",
            "  Downloading cycler-0.10.0-py2.py3-none-any.whl (6.5 kB)\n",
            "Collecting decorator==5.1.0 (from -r requirements.txt (line 3))\n",
            "  Downloading decorator-5.1.0-py3-none-any.whl (9.1 kB)\n",
            "Collecting face-alignment==1.3.5 (from -r requirements.txt (line 4))\n",
            "  Downloading face_alignment-1.3.5-py2.py3-none-any.whl (29 kB)\n",
            "Collecting imageio==2.9.0 (from -r requirements.txt (line 5))\n",
            "  Downloading imageio-2.9.0-py3-none-any.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m87.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting imageio-ffmpeg==0.4.5 (from -r requirements.txt (line 6))\n",
            "  Downloading imageio_ffmpeg-0.4.5-py3-none-manylinux2010_x86_64.whl (26.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.9/26.9 MB\u001b[0m \u001b[31m69.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting kiwisolver==1.3.2 (from -r requirements.txt (line 7))\n",
            "  Downloading kiwisolver-1.3.2-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m88.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting matplotlib==3.4.3 (from -r requirements.txt (line 8))\n",
            "  Downloading matplotlib-3.4.3.tar.gz (37.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.9/37.9 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting networkx==2.6.3 (from -r requirements.txt (line 9))\n",
            "  Downloading networkx-2.6.3-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m86.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting numpy==1.20.3 (from -r requirements.txt (line 10))\n",
            "  Downloading numpy-1.20.3.zip (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m114.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pandas==1.3.3 (from -r requirements.txt (line 11))\n",
            "  Downloading pandas-1.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m106.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting Pillow==8.3.2 (from -r requirements.txt (line 12))\n",
            "  Downloading Pillow-8.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m110.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pycparser==2.20 (from -r requirements.txt (line 13))\n",
            "  Downloading pycparser-2.20-py2.py3-none-any.whl (112 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.0/112.0 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyparsing==2.4.7 (from -r requirements.txt (line 14))\n",
            "  Downloading pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.8/67.8 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil==2.8.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 15)) (2.8.2)\n",
            "Collecting pytz==2021.1 (from -r requirements.txt (line 16))\n",
            "  Downloading pytz-2021.1-py2.py3-none-any.whl (510 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.8/510.8 kB\u001b[0m \u001b[31m51.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting PyWavelets==1.1.1 (from -r requirements.txt (line 17))\n",
            "  Downloading PyWavelets-1.1.1.tar.gz (4.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m98.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting PyYAML==5.4.1 (from -r requirements.txt (line 18))\n",
            "  Downloading PyYAML-5.4.1.tar.gz (175 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.1/175.1 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
            "\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "url = \"https://huggingface.co/spaces/AlekseyKorshuk/thin-plate-spline-motion-model/resolve/main/checkpoints/vox.pth.tar\"\n",
        "file_name = \"vox.pth.tar\"\n",
        "\n",
        "response = requests.get(url)\n",
        "\n",
        "if response.status_code == 200:\n",
        "    with open(file_name, 'wb') as f:\n",
        "        f.write(response.content)\n",
        "    print(f\"File '{file_name}' berhasil diunduh.\")\n",
        "else:\n",
        "    print(f\"Gagal mengunduh file dari URL: {url}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kw-tDRZKO7-5",
        "outputId": "29529b85-8cc3-419d-c733-36c878ed4ceb"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File 'vox.pth.tar' berhasil diunduh.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "0Dcv4l7IO8ZP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python run.py --config config/vox-256.yaml --device_ids 0,1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xnUhtml1Ntzr",
        "outputId": "28a230f0-3058-4d61-afa4-893dbc7cf26d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/content/Thin-Plate-Spline-Motion-Model/run.py\", line 17, in <module>\n",
            "    from train import train\n",
            "  File \"/content/Thin-Plate-Spline-Motion-Model/train.py\", line 4, in <module>\n",
            "    from logger import Logger\n",
            "  File \"/content/Thin-Plate-Spline-Motion-Model/logger.py\", line 7, in <module>\n",
            "    from skimage.draw import circle\n",
            "ImportError: cannot import name 'circle' from 'skimage.draw' (/usr/local/lib/python3.10/dist-packages/skimage/draw/__init__.py)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Use Here"
      ],
      "metadata": {
        "id": "U3Y7Y-xSYqHD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title image to video\n",
        "file_photo = \"./assets/tes.jpg\" # @param {type:\"string\"}\n",
        "file_video = \" ./assets/driving.mp4\" # @param {type:\"string\"}\n",
        "\n",
        "!python demo.py --config config/vox-256.yaml --checkpoint vox.pth.tar --source_image {file_photo} --driving_video{file_video}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "6f3VIAKVKj5G",
        "outputId": "ad0351c3-1f56-4d82-fc52-e52a3706bd5f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Thin-Plate-Spline-Motion-Model/demo.py:147: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
            "  source_image = imageio.imread(opt.source_image)\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "100% 169/169 [00:11<00:00, 15.32it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "# Manual Runing\n",
        "Config\n"
      ],
      "metadata": {
        "id": "8Syx9AN0XQLP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# edit the config\n",
        "device = torch.device('cuda:0')\n",
        "dataset_name = 'vox' # ['vox', 'taichi', 'ted', 'mgif']\n",
        "source_image_path = './assets/source.png'\n",
        "driving_video_path = './assets/driving.mp4'\n",
        "output_video_path = './generated.mp4'\n",
        "config_path = 'config/vox-256.yaml'\n",
        "checkpoint_path = 'checkpoints/vox.pth.tar'\n",
        "predict_mode = 'relative' # ['standard', 'relative', 'avd']\n",
        "find_best_frame = False # when use the relative mode to animate a face, use 'find_best_frame=True' can get better quality result\n",
        "\n",
        "pixel = 256 # for vox, taichi and mgif, the resolution is 256*256\n",
        "if(dataset_name == 'ted'): # for ted, the resolution is 384*384\n",
        "    pixel = 384"
      ],
      "metadata": {
        "id": "jcck2iKUXTYu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read image and video"
      ],
      "metadata": {
        "id": "H3nMA2CNYZt0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import imageio\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "from skimage.transform import resize\n",
        "from IPython.display import HTML\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "source_image = imageio.imread(source_image_path)\n",
        "reader = imageio.get_reader(driving_video_path)\n",
        "\n",
        "\n",
        "source_image = resize(source_image, (pixel, pixel))[..., :3]\n",
        "\n",
        "fps = reader.get_meta_data()['fps']\n",
        "driving_video = []\n",
        "try:\n",
        "    for im in reader:\n",
        "        driving_video.append(im)\n",
        "except RuntimeError:\n",
        "    pass\n",
        "reader.close()\n",
        "\n",
        "driving_video = [resize(frame, (pixel, pixel))[..., :3] for frame in driving_video]\n",
        "\n",
        "def display(source, driving, generated=None):\n",
        "    fig = plt.figure(figsize=(8 + 4 * (generated is not None), 6))\n",
        "\n",
        "    ims = []\n",
        "    for i in range(len(driving)):\n",
        "        cols = [source]\n",
        "        cols.append(driving[i])\n",
        "        if generated is not None:\n",
        "            cols.append(generated[i])\n",
        "        im = plt.imshow(np.concatenate(cols, axis=1), animated=True)\n",
        "        plt.axis('off')\n",
        "        ims.append([im])\n",
        "\n",
        "    ani = animation.ArtistAnimation(fig, ims, interval=50, repeat_delay=1000)\n",
        "    plt.close()\n",
        "    return ani\n",
        "\n",
        "\n",
        "HTML(display(source_image, driving_video).to_html5_video())"
      ],
      "metadata": {
        "id": "FYx10lg9YDuU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a model and load checkpoints"
      ],
      "metadata": {
        "id": "Xf6IZuJjYW0n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from demo import load_checkpoints\n",
        "inpainting, kp_detector, dense_motion_network, avd_network = load_checkpoints(config_path = config_path, checkpoint_path = checkpoint_path, device = device)"
      ],
      "metadata": {
        "id": "lmaIg9woYISY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perform image animation"
      ],
      "metadata": {
        "id": "vyU-UdZSYU-7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from demo import make_animation\n",
        "from skimage import img_as_ubyte\n",
        "\n",
        "if predict_mode=='relative' and find_best_frame:\n",
        "    from demo import find_best_frame as _find\n",
        "    i = _find(source_image, driving_video, device.type=='cpu')\n",
        "    print (\"Best frame: \" + str(i))\n",
        "    driving_forward = driving_video[i:]\n",
        "    driving_backward = driving_video[:(i+1)][::-1]\n",
        "    predictions_forward = make_animation(source_image, driving_forward, inpainting, kp_detector, dense_motion_network, avd_network, device = device, mode = predict_mode)\n",
        "    predictions_backward = make_animation(source_image, driving_backward, inpainting, kp_detector, dense_motion_network, avd_network, device = device, mode = predict_mode)\n",
        "    predictions = predictions_backward[::-1] + predictions_forward[1:]\n",
        "else:\n",
        "    predictions = make_animation(source_image, driving_video, inpainting, kp_detector, dense_motion_network, avd_network, device = device, mode = predict_mode)\n",
        "\n",
        "#save resulting video\n",
        "imageio.mimsave(output_video_path, [img_as_ubyte(frame) for frame in predictions], fps=fps)\n",
        "\n",
        "HTML(display(source_image, driving_video, predictions).to_html5_video())"
      ],
      "metadata": {
        "id": "bI27MBojYNrU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}